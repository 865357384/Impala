# distinct *
select distinct *
from functional.testtbl
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  1:AGGREGATE
  |  group by: functional.testtbl.id, functional.testtbl.name, functional.testtbl.zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  4:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 3>, <slot 4>, <slot 5>

  STREAM DATA SINK
    EXCHANGE ID: 4
    UNPARTITIONED

  3:AGGREGATE
  |  group by: <slot 3>, <slot 4>, <slot 5>
  |  tuple ids: 1 
  |  
  2:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 2
    HASH_PARTITIONED: <slot 3>, <slot 4>, <slot 5>

  1:AGGREGATE
  |  group by: functional.testtbl.id, functional.testtbl.name, functional.testtbl.zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
====
# distinct w/ explicit select list
select distinct id, zip
from functional.testtbl
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  1:AGGREGATE
  |  group by: id, zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  4:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 2>, <slot 3>

  STREAM DATA SINK
    EXCHANGE ID: 4
    UNPARTITIONED

  3:AGGREGATE
  |  group by: <slot 2>, <slot 3>
  |  tuple ids: 1 
  |  
  2:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 2
    HASH_PARTITIONED: <slot 2>, <slot 3>

  1:AGGREGATE
  |  group by: id, zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
====
# count(distinct)
select count(distinct id, zip)
from functional.testtbl
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(if(<slot 2> IS NULL, NULL, <slot 3>))
  |  group by: 
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  group by: id, zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  6:AGGREGATE
  |  output: SUM(<slot 4>)
  |  group by: 
  |  tuple ids: 2 
  |  
  5:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 2>, <slot 3>

  STREAM DATA SINK
    EXCHANGE ID: 5
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(if(<slot 2> IS NULL, NULL, <slot 3>))
  |  group by: 
  |  tuple ids: 2 
  |  
  4:AGGREGATE
  |  group by: <slot 2>, <slot 3>
  |  tuple ids: 1 
  |  
  3:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 3
    HASH_PARTITIONED: <slot 2>, <slot 3>

  1:AGGREGATE
  |  group by: id, zip
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.testtbl #partitions=0 size=0B
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
====
# count(distinct) w/ grouping
select tinyint_col, count(distinct int_col, bigint_col)
from functional.alltypesagg
group by 1
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(if(<slot 4> IS NULL, NULL, <slot 5>))
  |  group by: <slot 3>
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  group by: tinyint_col, int_col, bigint_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  5:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 3>

  STREAM DATA SINK
    EXCHANGE ID: 5
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(if(<slot 4> IS NULL, NULL, <slot 5>))
  |  group by: <slot 3>
  |  tuple ids: 2 
  |  
  4:AGGREGATE
  |  group by: <slot 3>, <slot 4>, <slot 5>
  |  tuple ids: 1 
  |  
  3:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 3
    HASH_PARTITIONED: <slot 3>

  1:AGGREGATE
  |  group by: tinyint_col, int_col, bigint_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=1/100101.txt 0:75153
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=10/100110.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=2/100102.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=3/100103.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=4/100104.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=5/100105.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=6/100106.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=7/100107.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=8/100108.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=9/100109.txt 0:76263
====
# count(distinct) and sum(distinct) w/ grouping
select tinyint_col, count(distinct int_col), sum(distinct int_col)
from functional.alltypesagg
group by 1
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 3>), SUM(<slot 3>)
  |  group by: <slot 2>
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  5:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 2>

  STREAM DATA SINK
    EXCHANGE ID: 5
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 3>), SUM(<slot 3>)
  |  group by: <slot 2>
  |  tuple ids: 2 
  |  
  4:AGGREGATE
  |  group by: <slot 2>, <slot 3>
  |  tuple ids: 1 
  |  
  3:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 3
    HASH_PARTITIONED: <slot 2>

  1:AGGREGATE
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=1/100101.txt 0:75153
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=10/100110.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=2/100102.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=3/100103.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=4/100104.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=5/100105.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=6/100106.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=7/100107.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=8/100108.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=9/100109.txt 0:76263
====
# count(distinct) and sum(distinct) w/ grouping; distinct in min() and max()
# is ignored
select tinyint_col, count(distinct int_col),
min(distinct smallint_col), max(distinct string_col)
from functional.alltypesagg group by 1
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 5>), MIN(<slot 6>), MAX(<slot 7>)
  |  group by: <slot 4>
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  output: MIN(smallint_col), MAX(string_col)
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  5:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 4>

  STREAM DATA SINK
    EXCHANGE ID: 5
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 5>), MIN(<slot 6>), MAX(<slot 7>)
  |  group by: <slot 4>
  |  tuple ids: 2 
  |  
  4:AGGREGATE
  |  output: MIN(<slot 6>), MAX(<slot 7>)
  |  group by: <slot 4>, <slot 5>
  |  tuple ids: 1 
  |  
  3:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 3
    HASH_PARTITIONED: <slot 4>

  1:AGGREGATE
  |  output: MIN(smallint_col), MAX(string_col)
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=1/100101.txt 0:75153
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=10/100110.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=2/100102.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=3/100103.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=4/100104.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=5/100105.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=6/100106.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=7/100107.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=8/100108.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=9/100109.txt 0:76263
====
# aggregate fns with and without distinct
select tinyint_col, count(distinct int_col), count(*), sum(distinct int_col),
sum(int_col), min(smallint_col), max(bigint_col)
from functional.alltypesagg group by 1
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 5>), SUM(<slot 5>), SUM(<slot 6>), SUM(<slot 7>), MIN(<slot 8>), MAX(<slot 9>)
  |  group by: <slot 4>
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  output: COUNT(*), SUM(int_col), MIN(smallint_col), MAX(bigint_col)
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  5:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: HASH_PARTITIONED: <slot 4>

  STREAM DATA SINK
    EXCHANGE ID: 5
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 5>), SUM(<slot 5>), SUM(<slot 6>), SUM(<slot 7>), MIN(<slot 8>), MAX(<slot 9>)
  |  group by: <slot 4>
  |  tuple ids: 2 
  |  
  4:AGGREGATE
  |  output: SUM(<slot 6>), SUM(<slot 7>), MIN(<slot 8>), MAX(<slot 9>)
  |  group by: <slot 4>, <slot 5>
  |  tuple ids: 1 
  |  
  3:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 2
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 3
    HASH_PARTITIONED: <slot 4>

  1:AGGREGATE
  |  output: COUNT(*), SUM(int_col), MIN(smallint_col), MAX(bigint_col)
  |  group by: tinyint_col, int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypesagg #partitions=10 size=743.67KB
     tuple ids: 0 
---- SCANRANGELOCATIONS
NODE 0:
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=1/100101.txt 0:75153
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=10/100110.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=2/100102.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=3/100103.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=4/100104.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=5/100105.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=6/100106.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=7/100107.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=8/100108.txt 0:76263
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypesagg/year=2010/month=1/day=9/100109.txt 0:76263
====
# test join on inline views containing distinct aggregates to make sure
# the aggregation info reports the correct tuple ids (from the 2nd phase
# distinct aggregation) for the inline-view expression substitution
select t1.c, t2.c from
(select count(distinct int_col) as c from functional.alltypestiny) t1 inner join
(select count(distinct bigint_col) as c from functional.alltypestiny) t2 on (t1.c = t2.c)
---- PLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  6:HASH JOIN
  |  join op: INNER JOIN
  |  hash predicates:
  |    <slot 2> = <slot 6>
  |  tuple ids: 2 6 
  |  
  |----5:AGGREGATE
  |    |  output: COUNT(<slot 5>)
  |    |  group by: 
  |    |  tuple ids: 6 
  |    |  
  |    4:AGGREGATE
  |    |  group by: bigint_col
  |    |  tuple ids: 5 
  |    |  
  |    3:SCAN HDFS
  |       table=functional.alltypestiny #partitions=4 size=460B
  |       tuple ids: 4 
  |    
  2:AGGREGATE
  |  output: COUNT(<slot 1>)
  |  group by: 
  |  tuple ids: 2 
  |  
  1:AGGREGATE
  |  group by: int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypestiny #partitions=4 size=460B
     tuple ids: 0 
---- DISTRIBUTEDPLAN
PLAN FRAGMENT 0
  PARTITION: UNPARTITIONED

  6:HASH JOIN
  |  join op: INNER JOIN (BROADCAST)
  |  hash predicates:
  |    <slot 2> = <slot 6>
  |  tuple ids: 2 6 
  |  
  |----15:EXCHANGE
  |       tuple ids: 6 
  |    
  10:AGGREGATE
  |  output: SUM(<slot 2>)
  |  group by: 
  |  tuple ids: 2 
  |  
  9:EXCHANGE
     tuple ids: 2 

PLAN FRAGMENT 1
  PARTITION: UNPARTITIONED

  STREAM DATA SINK
    EXCHANGE ID: 15
    UNPARTITIONED

  14:AGGREGATE
  |  output: SUM(<slot 6>)
  |  group by: 
  |  tuple ids: 6 
  |  
  13:EXCHANGE
     tuple ids: 6 

PLAN FRAGMENT 2
  PARTITION: HASH_PARTITIONED: <slot 5>

  STREAM DATA SINK
    EXCHANGE ID: 13
    UNPARTITIONED

  5:AGGREGATE
  |  output: COUNT(<slot 5>)
  |  group by: 
  |  tuple ids: 6 
  |  
  12:AGGREGATE
  |  group by: <slot 5>
  |  tuple ids: 5 
  |  
  11:EXCHANGE
     tuple ids: 5 

PLAN FRAGMENT 3
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 11
    HASH_PARTITIONED: <slot 5>

  4:AGGREGATE
  |  group by: bigint_col
  |  tuple ids: 5 
  |  
  3:SCAN HDFS
     table=functional.alltypestiny #partitions=4 size=460B
     tuple ids: 4 

PLAN FRAGMENT 4
  PARTITION: HASH_PARTITIONED: <slot 1>

  STREAM DATA SINK
    EXCHANGE ID: 9
    UNPARTITIONED

  2:AGGREGATE
  |  output: COUNT(<slot 1>)
  |  group by: 
  |  tuple ids: 2 
  |  
  8:AGGREGATE
  |  group by: <slot 1>
  |  tuple ids: 1 
  |  
  7:EXCHANGE
     tuple ids: 1 

PLAN FRAGMENT 5
  PARTITION: RANDOM

  STREAM DATA SINK
    EXCHANGE ID: 7
    HASH_PARTITIONED: <slot 1>

  1:AGGREGATE
  |  group by: int_col
  |  tuple ids: 1 
  |  
  0:SCAN HDFS
     table=functional.alltypestiny #partitions=4 size=460B
     tuple ids: 0 
---- SCANRANGELOCATIONS
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=1/090101.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=1/090101.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=2/090201.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=2/090201.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=3/090301.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=3/090301.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=4/090401.txt 0:115
  HDFS SPLIT hdfs://localhost:20500/test-warehouse/alltypestiny/year=2009/month=4/090401.txt 0:115
NODE 0:
NODE 3:
====